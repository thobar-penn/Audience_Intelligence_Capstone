{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5438df8c-c58d-49f1-8600-ccd0454e0cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0) Config - Imports, Spark and MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913b57d1-91f6-4765-963c-2df974ec1840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os, matplotlib.pyplot as plt, mlflow, math, random, os\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorSlicer\n",
    "from synapse.ml.lightgbm import LightGBMRegressor as LGBReg\n",
    "from pyspark.sql.types import NumericType, StringType, BooleanType\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from mlflow.spark import log_model\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34437f86-1ab2-4d64-8d84-261a836b98c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CRASH_SAFE             = True\n",
    "\n",
    "CV_TRIALS              = 10\n",
    "EARLY_STOP_ROUNDS      = 15\n",
    "\n",
    "OUT_BASE               = \"XXX\"\n",
    "IMG_BASE               = \"XXX\"\n",
    "DBFS_FILES_BASE        = \"XXX\"\n",
    "\n",
    "os.makedirs(IMG_BASE, exist_ok=True)\n",
    "os.makedirs(DBFS_FILES_BASE, exist_ok=True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 64*1024*1024)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 160 if CRASH_SAFE else 256)\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/capstone_ai_chkpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b2e3dc-282d-4817-a3b4-599b907e170e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ML Flow config\n",
    "MLFLOW_EXPERIMENT = \"XXX\"\n",
    "MODEL_NAME        = \"quality_model\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "# Source table and key cols\n",
    "SOURCE_TABLE        = \"XXX\"\n",
    "ID_COL              = \"XXX\"\n",
    "EXPOSURE_COL        = \"XXX\"\n",
    "LOSS_COL            = \"XXX\"\n",
    "LOSS_RATIO_TARGET   = \"XXX\"\n",
    "STATE_COL           = \"XXX\"\n",
    "\n",
    "RANDOM_SEED        = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "PRIMARY_METRIC     = \"mae_w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d53a034-555a-4f3b-9688-994c16bf2294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1) Define Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913ac170-7e59-402e-8176-16b5893e15da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split date features into date part int cols\n",
    "def add_date_features(df, col):\n",
    "    d=F.to_date(F.col(col))\n",
    "\n",
    "    return (df.withColumn(f\"{col}_year\",F.year(d).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_month\",F.month(d).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_dow\",F.dayofweek(d).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_dom\",F.dayofmonth(d).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_week\",F.weekofyear(d).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_is_month_end\",(F.dayofmonth(d)==F.dayofmonth(F.last_day(d))).cast(\"int\"))\n",
    "            .withColumn(f\"{col}_age_days\",F.datediff(F.current_date(), d).cast(\"int\")))\n",
    "\n",
    "# Random split by\n",
    "def random_split_by_state(sdf, state_col=\"State\", train=0.6, test=0.2, val=0.2, seed=42):\n",
    "    w = Window.partitionBy(state_col)\n",
    "    sdf2 = sdf.withColumn(\"_u\", F.rand(seed)) \n",
    "\n",
    "    b1, b2 = train, train + test\n",
    "\n",
    "    tr = sdf2.where(F.col(\"_u\") < b1).drop(\"_u\")\n",
    "    te = sdf2.where((F.col(\"_u\") >= b1) & (F.col(\"_u\") < b2)).drop(\"_u\")\n",
    "    va = sdf2.where(F.col(\"_u\") >= b2).drop(\"_u\")\n",
    "\n",
    "    return tr, te, va\n",
    "\n",
    "# Apply frequency encoding to high cardinality categorical features\n",
    "def apply_freq_encoding(df_in):\n",
    "    df_out = df_in\n",
    "    for c in cat_high:\n",
    "        cnt_df, cnt_col = cnt_maps[c]\n",
    "        freq_col = f\"{c}_freq\"\n",
    "        df_out = (\n",
    "            df_out.join(cnt_df, c, \"left\")\n",
    "                  .withColumn(freq_col, F.col(cnt_col) / F.lit(train_n))\n",
    "                  .drop(cnt_col)\n",
    "        )\n",
    "        if freq_col not in high_encoded_cols:\n",
    "            high_encoded_cols.append(freq_col)\n",
    "    return df_out\n",
    "\n",
    "# Extract mapping from vector index to get original input col name\n",
    "def get_feature_index_map(df, features_col=\"features\"):\n",
    "    meta = df.schema[features_col].metadata\n",
    "    attrs = meta[\"ml_attr\"][\"attrs\"]\n",
    "    all_attrs = []\n",
    "    for t in [\"binary\", \"numeric\", \"nominal\"]:\n",
    "        all_attrs.extend(attrs.get(t, []))\n",
    "\n",
    "    index_to_name = {a[\"idx\"]: a[\"name\"] for a in all_attrs}\n",
    "    return index_to_name\n",
    "\n",
    "# Remove OHE feature value suffix from feature name for base feature importance ranking\n",
    "def strip_ohe_suffix(name: str) -> str:\n",
    "    if name is None:\n",
    "        return None\n",
    "    pos = name.find(\"_oh\")\n",
    "    if pos == -1:\n",
    "        return name\n",
    "    return name[:pos+3]\n",
    "    \n",
    "strip_ohe_suffix_udf = F.udf(strip_ohe_suffix)\n",
    "\n",
    "# Calculate weighted baselines\n",
    "def weighted_baseline_metrics(sdf, label_col, w_col):\n",
    "    y  = F.col(label_col)\n",
    "    ww = F.col(w_col)\n",
    "\n",
    "    agg = sdf.agg(\n",
    "        F.sum(y * ww).alias(\"sum_y_w\"),\n",
    "        F.sum((y * y) * ww).alias(\"sum_y2_w\"),\n",
    "        F.sum(ww).alias(\"sum_w\")\n",
    "    ).first().asDict()\n",
    "\n",
    "    sum_w   = float(agg[\"sum_w\"]) if agg[\"sum_w\"] is not None else 0.0\n",
    "    sum_y_w = float(agg[\"sum_y_w\"]) if agg[\"sum_y_w\"] is not None else 0.0\n",
    "    sum_y2_w = float(agg[\"sum_y2_w\"]) if agg[\"sum_y2_w\"] is not None else 0.0\n",
    "\n",
    "    if sum_w == 0.0:\n",
    "        return {\n",
    "            \"baseline_mean\": float(\"nan\"),\n",
    "            \"baseline_mae_w\": float(\"nan\"),\n",
    "            \"baseline_mse_w\": float(\"nan\"),\n",
    "            \"baseline_rmse_w\": float(\"nan\")\n",
    "        }\n",
    "\n",
    "# Calculate metrics\n",
    "def metrics_sdf(sdf, label, pred, w):\n",
    "    y    = F.col(label)\n",
    "    yhat = F.col(pred)\n",
    "    ww   = F.col(w)\n",
    "\n",
    "    r = (\n",
    "        sdf.agg(\n",
    "            (F.sum(F.abs(y - yhat) * ww) / F.sum(ww)).alias(\"mae_w\"),\n",
    "            (F.sum(((y - yhat) * (y - yhat)) * ww) / F.sum(ww)).alias(\"mse_w\"),\n",
    "            F.sqrt(F.sum(((y - yhat) * (y - yhat)) * ww) / F.sum(ww)).alias(\"rmse_w\")\n",
    "        )\n",
    "        .first()\n",
    "        .asDict()\n",
    "    )\n",
    "\n",
    "    return r\n",
    "\n",
    "# Define model param space\n",
    "def param_samples(lib: str = \"xgb\", trials: int = 5):\n",
    "    if lib == \"xgb\":\n",
    "        for _ in range(trials):\n",
    "            yield dict(\n",
    "                max_depth        = random.choice([4, 6, 8]),\n",
    "                learning_rate    = random.choice([0.03, 0.06, 0.1]),\n",
    "                subsample        = random.choice([0.7, 0.8, 0.9]),\n",
    "                colsample_bytree = random.choice([0.6, 0.7, 0.8]),\n",
    "                reg_alpha        = random.choice([0.0, 0.1, 0.5]),\n",
    "                reg_lambda       = random.choice([0.5, 1.0, 2.0]),\n",
    "                n_estimators     = random.choice([300, 600, 1000]),\n",
    "            )\n",
    "    else:  # LightGBM\n",
    "        for _ in range(trials):\n",
    "            yield dict(\n",
    "                numLeaves       = random.choice([31, 63, 127]),\n",
    "                learningRate    = random.choice([0.03, 0.06, 0.1]),\n",
    "                baggingFraction = random.choice([0.7, 0.8, 0.9]),\n",
    "                featureFraction = random.choice([0.6, 0.7, 0.8]),\n",
    "                maxDepth        = random.choice([-1, 10, 20]),\n",
    "                numIterations   = random.choice([300, 600, 1000]))\n",
    "\n",
    "# Define CV evaluator\n",
    "def cv_fit_eval(model, ds, val, label, weight, run_name):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        m = model.fit(ds)\n",
    "\n",
    "        # score on validation set\n",
    "        va_scored = (\n",
    "            m.transform(\n",
    "                val.select(\n",
    "                    \"features\",\n",
    "                    F.col(label).alias(label),\n",
    "                    F.col(weight).alias(weight),\n",
    "                )\n",
    "            )\n",
    "            .select(\n",
    "                F.col(\"prediction\").alias(\"pred\"),\n",
    "                F.col(label).alias(\"y\"),\n",
    "                F.col(weight).alias(\"w\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        mets = metrics_sdf(\n",
    "            va_scored.withColumnRenamed(\"y\", label).withColumnRenamed(\"pred\", \"yhat\"),\n",
    "            label,\n",
    "            \"yhat\",\n",
    "            \"w\",\n",
    "        )\n",
    "\n",
    "        for k, v in mets.items():\n",
    "            mlflow.log_metric(f\"cv_{k}\", float(v))\n",
    "\n",
    "    return m, mets\n",
    "\n",
    "# Execute and identify best trials\n",
    "def best_of_trials(train, val, label, weight, run_prefix=\"cv\", objective=\"tweedie\"):\n",
    "\n",
    "    best_m, best_s, best_tag = None, {\"mae_w\": float(\"inf\")}, None\n",
    "\n",
    "    tr = train.withColumn(\"is_val\", F.lit(False))\n",
    "    va = val.withColumn(\"is_val\", F.lit(True))\n",
    "    parts_local = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "    ds = tr.unionByName(va).repartition(parts_local).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    _ = ds.count()\n",
    "\n",
    "    trials = CV_TRIALS\n",
    "\n",
    "    # LightGBM\n",
    "    for i, ps in enumerate(param_samples(\"lgbm\", trials), 1):\n",
    "        params = dict(\n",
    "            labelCol=label,\n",
    "            featuresCol=\"features\",\n",
    "            validationIndicatorCol=\"is_val\",\n",
    "            earlyStoppingRound=EARLY_STOP_ROUNDS,\n",
    "            seed=RANDOM_SEED,\n",
    "            objective=(\"tweedie\" if objective == \"tweedie\" else \"regression\"),\n",
    "            metric=\"l1\",\n",
    "        )\n",
    "        params.update(ps)\n",
    "        model = LGBReg(**{k: v for k, v in params.items() if v is not None})\n",
    "\n",
    "        m, s = cv_fit_eval(\n",
    "            model,\n",
    "            ds,\n",
    "            val,\n",
    "            label,\n",
    "            weight,\n",
    "            run_name=f\"{run_prefix}_lgbm_{i}\",\n",
    "        )\n",
    "\n",
    "        if s[\"mae_w\"] < best_s[\"mae_w\"]:\n",
    "            best_m, best_s, best_tag = m, s, \"lgbm\"\n",
    "        spark.catalog.clearCache()\n",
    "\n",
    "    # XGBoost\n",
    "    num_cores   = spark.sparkContext.defaultParallelism\n",
    "    num_workers = max(2, num_cores // 2)\n",
    "\n",
    "    for i, ps in enumerate(param_samples(\"xgb\", trials), 1):\n",
    "        params = dict(\n",
    "            features_col=\"features\",\n",
    "            label_col=label,\n",
    "            validation_indicator_col=\"is_val\",\n",
    "            early_stopping_rounds=EARLY_STOP_ROUNDS,\n",
    "            missing=float(\"nan\"),\n",
    "            num_workers=num_workers,\n",
    "            tree_method=\"hist\",\n",
    "            grow_policy=\"lossguide\",\n",
    "            max_bin=256,\n",
    "            objective=(\"reg:tweedie\" if objective == \"tweedie\" else \"reg:squarederror\"),\n",
    "            eval_metric=\"mae\",\n",
    "        )\n",
    "        params.update(ps)\n",
    "        model = SparkXGBRegressor(**params)\n",
    "\n",
    "        m, s = cv_fit_eval(\n",
    "            model,\n",
    "            ds,\n",
    "            val,\n",
    "            label,\n",
    "            weight,\n",
    "            run_name=f\"{run_prefix}_xgb_{i}\",\n",
    "        )\n",
    "\n",
    "        if s[\"mae_w\"] < best_s[\"mae_w\"]:\n",
    "            best_m, best_s, best_tag = m, s, \"xgb\"\n",
    "        spark.catalog.clearCache()\n",
    "\n",
    "    ds.unpersist()\n",
    "\n",
    "    return best_tag, best_m, best_s\n",
    "        \n",
    "# Evaluate best model\n",
    "def eval_and_save(tag, label_col, pred_alias):\n",
    "\n",
    "    global best_model\n",
    "\n",
    "    model = best_model\n",
    "\n",
    "    # Score\n",
    "    test_pred = model.transform(\n",
    "        dfe_te.select(\n",
    "            \"features\",\n",
    "            F.col(label_col).alias(label_col),\n",
    "            F.col(\"model_weight\").alias(\"mw\"),\n",
    "        )\n",
    "    ).select(\n",
    "        F.col(\"prediction\").alias(pred_alias),\n",
    "        F.col(label_col).alias(\"y\"),\n",
    "        F.col(\"mw\").alias(\"w\"),\n",
    "    )\n",
    "\n",
    "    # Metrics vs baseline \n",
    "    mets = metrics_sdf(test_pred, \"y\", pred_alias, \"w\")\n",
    "    base = weighted_baseline_metrics(\n",
    "        test_pred.select(F.col(\"y\").alias(\"y\"), F.col(\"w\").alias(\"w\")),\n",
    "        \"y\",\n",
    "        \"w\",\n",
    "    )\n",
    "    rel_improve = 1.0 - (mets[\"mae_w\"] / max(base[\"baseline_mae_w\"], 1e-9))\n",
    "\n",
    "    # Save preds & deciles\n",
    "    test_pred.write.mode(\"overwrite\").parquet(f\"{OUT_BASE}/{tag}_test_pred\")\n",
    "    deciles_and_lift(\n",
    "        test_pred.select(F.col(pred_alias).alias(\"pred\"), F.col(\"y\"), F.col(\"w\")),\n",
    "        \"pred\",\n",
    "        \"y\",\n",
    "        \"w\",\n",
    "    ).write.mode(\"overwrite\").parquet(f\"{OUT_BASE}/{tag}_test_lift\")\n",
    "\n",
    "    # Plot MAE vs baseline\n",
    "    plt.figure()\n",
    "    plt.bar(\n",
    "        [\"Baseline MAE_w\", \"Model MAE_w\"],\n",
    "        [base[\"baseline_mae_w\"], mets[\"mae_w\"]],\n",
    "    )\n",
    "    plt.ylabel(\"Exposure-weighted MAE\")\n",
    "    plt.title(f\"{tag.upper()} — Test MAE vs Baseline\")\n",
    "    p1 = f\"{IMG_BASE}/{tag}_mae_vs_baseline.png\"\n",
    "    plt.savefig(p1, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot deciles calibration\n",
    "    lift_df = spark.read.parquet(f\"{OUT_BASE}/{tag}_test_lift\").orderBy(\"decile\").toPandas()\n",
    "    p2 = f\"{IMG_BASE}/{tag}_deciles.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(lift_df[\"decile\"], lift_df[\"actual\"], label=\"Actual\")\n",
    "    plt.plot(lift_df[\"decile\"], lift_df[\"pred\"], label=\"Predicted\")\n",
    "    plt.xlabel(\"Decile\")\n",
    "    plt.ylabel(label_col)\n",
    "    plt.title(f\"{tag.upper()} — Decile Calibration\")\n",
    "    plt.legend()\n",
    "    plt.savefig(p2, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return dict(mets=mets, base=base, rel_improve=rel_improve, plots=[p1, p2] + seg_paths)\n",
    "#Calculate deciles and lift\n",
    "def deciles_and_lift(sdf, pred_col, label_col, w_col):\n",
    "    qs = [i/10.0 for i in range(1,10)]\n",
    "    quantiles = sdf.approxQuantile(pred_col, qs, 1e-3)\n",
    "    bounds = [-float(\"inf\")] + quantiles + [float(\"inf\")]\n",
    "    expr = None\n",
    "    for i in range(10):\n",
    "        low, high = bounds[i], bounds[i+1]\n",
    "        cond = (F.col(pred_col) > F.lit(low)) & (F.col(pred_col) <= F.lit(high))\n",
    "        expr = cond.cast(\"int\")*F.lit(i+1) if expr is None else F.when(cond, i+1).otherwise(expr)\n",
    "    scored = sdf.withColumn(\"decile\", expr)\n",
    "    w = F.col(w_col)\n",
    "    agg = (scored.groupBy(\"decile\")\n",
    "           .agg(F.sum(w).alias(\"w\"),\n",
    "                F.sum(F.col(label_col)*w).alias(\"wy\"),\n",
    "                F.sum(F.col(pred_col)*w).alias(\"wyp\")))\n",
    "    return agg.select(\"decile\",\n",
    "                      (F.col(\"wy\")/F.col(\"w\")).alias(\"actual\"),\n",
    "                      (F.col(\"wyp\")/F.col(\"w\")).alias(\"pred\"),\n",
    "                      \"w\").orderBy(\"decile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c79dfa4-a709-4a09-9a45-14beedcde100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5bf074-f8bc-46fe-9b1a-06fdb6d37dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] rows=522,026 | shuffle.partitions=128\n"
     ]
    }
   ],
   "source": [
    "# Load table\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "\n",
    "# Shuttle partition optimziation\n",
    "row_est = df.count()\n",
    "parts = max(32, min(200, int(math.ceil(row_est / 300_000.0) * 64)))\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", parts)\n",
    "print(f\"[Config] rows={row_est:,} | shuffle.partitions={parts}\")\n",
    "\n",
    "# Valid exposure rows\n",
    "df = df.where((F.col(EXPOSURE_COL) > 0) & (~F.isnan(F.col(EXPOSURE_COL))) & F.col(EXPOSURE_COL).isNotNull())\n",
    "\n",
    "# Derived targets\n",
    "df = df.withColumn(\"offset_log_exposure\", F.log(F.col(EXPOSURE_COL)))\n",
    "\n",
    "# Parse date cols\n",
    "date_cols = [c for c,t in df.dtypes if t in (\"date\",\"timestamp\")]\n",
    "\n",
    "for dc in date_cols: \n",
    "    df = add_date_features(df, dc)\n",
    "\n",
    "df = df.drop(*date_cols)\n",
    "\n",
    "# Encode bool cols as string (1 and null values only, no 0)\n",
    "bool_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, BooleanType)]\n",
    "\n",
    "yn_true = {\"Y\",\"1\",\"true\",\"True\"} \n",
    "\n",
    "for c in bool_cols:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c).isNull(), F.lit(\"__MISSING__\"))\n",
    "         .when(F.lower(F.col(c)).isin(*yn_true), F.lit(\"True\"))\n",
    "         .otherwise(F.lit(\"False\")))\n",
    "\n",
    "# Define exposure length-based weight\n",
    "df = df.withColumn(\"model_weight\", F.col(EXPOSURE_COL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8830535-6cb3-4156-88cf-c3339850f7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3) State-Aware Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cbb7c2-82b6-4b6a-860e-cf6d3c2e0b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# State-aware random train/test/val split\n",
    "train_df, test_df, val_df = random_split_by_state(df, state_col=STATE_COL, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3374e945-67e9-493e-842e-8e641d1e70f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4) Preprocessing (Feature Encoding, Vector Assembly, Materialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9e9eaa-13bc-4b2d-8879-8be0cec45696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: nvirginia.cloud.databricks.com. Connection pool size: 10\n"
     ]
    }
   ],
   "source": [
    "# Exclusion cols (IDs, targets)\n",
    "EXCLUDE = {\n",
    "    ID_COL,\n",
    "    STATE_COL,\n",
    "    EXPOSURE_COL,\n",
    "    LOSS_RATIO_TARGET,\n",
    "    LOSS_COL,\n",
    "    \"offset_log_exposure\",\n",
    "    \"model_weight\",\n",
    "    \"state_weight\",\n",
    "    \"EmailAddress\"\n",
    "    \"tier\",\n",
    "    \"credit_score\",\n",
    "    \"tenure_years\",\n",
    "    \"renewal_month\"\n",
    "}\n",
    "\n",
    "# Identify col datatype\n",
    "numeric_cols = [\n",
    "    f.name for f in train_df.schema.fields\n",
    "    if isinstance(f.dataType, NumericType) and f.name not in EXCLUDE ]\n",
    "\n",
    "string_cols = [\n",
    "    f.name for f in train_df.schema.fields\n",
    "    if isinstance(f.dataType, StringType) and f.name not in EXCLUDE ]\n",
    "\n",
    "# Identify low-card vs high-card categoricals\n",
    "row_counts = train_df.select([F.approx_count_distinct(c) for c in string_cols]).collect()[0]\n",
    "card_row = {c: row_counts[i] for i, c in enumerate(string_cols)}\n",
    "\n",
    "LOW_CARD_MAX = 40  # threshold for OHE\n",
    "\n",
    "cat_low  = [c for c in string_cols if card_row[c] <= LOW_CARD_MAX] \n",
    "cat_high = [c for c in string_cols if card_row[c] > LOW_CARD_MAX]\n",
    "\n",
    "# Frequency-encode high-card categoricals\n",
    "train_n = train_df.count()\n",
    "high_encoded_cols = []\n",
    "\n",
    "cnt_maps = {}\n",
    "for c in cat_high:\n",
    "    cnt_col = f\"{c}_cnt\"\n",
    "    cnt_maps[c] = (\n",
    "        train_df.groupBy(c).agg(F.count(\"*\").alias(cnt_col)),\n",
    "        cnt_col\n",
    "    )\n",
    "\n",
    "dfe_tr_enc = apply_freq_encoding(train_df) \n",
    "dfe_te_enc = apply_freq_encoding(test_df) \n",
    "dfe_va_enc = apply_freq_encoding(val_df)\n",
    "\n",
    "# StringIndexer + OneHotEncoder for Llow-card categoricals\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_low\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_oh\")\n",
    "    for c in cat_low\n",
    "]\n",
    "\n",
    "ohe_cols = [f\"{c}_oh\" for c in cat_low]\n",
    "\n",
    "# Assemble feature vector and define pipeline\n",
    "feature_cols = numeric_cols + high_encoded_cols + ohe_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Fit preprocessing model\n",
    "prep_model = pipe.fit(dfe_tr_enc)\n",
    "\n",
    "# Transform train/test/val splits\n",
    "dfe_tr = prep_model.transform(dfe_tr_enc)\n",
    "dfe_te = prep_model.transform(dfe_te_enc) \n",
    "dfe_va = prep_model.transform(dfe_va_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6da258-a030-43bc-9b46-8ac930088492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5) Log Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c82ddc5-b33c-45fc-8bd0-a4070623b85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "# Calculate and log baseline metrics for each split\n",
    "splits = {\n",
    "    \"train\": dfe_tr.select(LOSS_RATIO_TARGET, \"model_weight\"),\n",
    "    \"test\":  dfe_te.select(LOSS_RATIO_TARGET, \"model_weight\"),\n",
    "    \"val\":   dfe_va.select(LOSS_RATIO_TARGET, \"model_weight\"),\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"capstone_baselines\"):\n",
    "\n",
    "    for split_name, sdf in splits.items():\n",
    "            base = weighted_baseline_metrics(\n",
    "                sdf.where(F.col(LOSS_RATIO_TARGET).isNotNull()),\n",
    "                LOSS_RATIO_TARGET,\n",
    "                \"model_weight\",\n",
    "            )\n",
    "            for k, v in base.items():\n",
    "                mlflow.log_metric(f\"{split_name}_loss_ratio_{k}\", float(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9b5725-2006-4efc-8a6d-6e78a76c7e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6) Fit Fixed Regression Model for Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d07f7a3-271e-4af8-abeb-cdc2077bdce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FS] Fixed model (all features) – val metrics:\n",
      "      mae          = 0.08394603081957387\n",
      "      mse          = 0.08013010663960451\n",
      "      rmse          = 0.28307261725501554\n"
     ]
    }
   ],
   "source": [
    "# Define feature selection splits\n",
    "train_fs = dfe_tr.where(F.col(LOSS_RATIO_TARGET).isNotNull())\n",
    "val_fs   = dfe_va.where(F.col(LOSS_RATIO_TARGET).isNotNull())\n",
    "\n",
    "# Map feature index -> name\n",
    "index_to_name = get_feature_index_map(train_fs, features_col=\"features\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"fs_fixed_lgb_loss_ratio\"):\n",
    "    # Train fixed LightGBM model for feature importances\n",
    "    lgb_fs_params = dict(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=LOSS_RATIO_TARGET,\n",
    "        objective=\"regression\",\n",
    "        numIterations=300,\n",
    "        learningRate=0.05,\n",
    "        numLeaves=31,\n",
    "        maxDepth=-1,\n",
    "        baggingFraction=0.8,\n",
    "        featureFraction=0.8,\n",
    "        seed=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    # Log FS model hyperparams\n",
    "    for p, v in lgb_fs_params.items():\n",
    "        mlflow.log_param(f\"fs_{p}\", v)\n",
    "    mlflow.log_param(\"fs_target\", LOSS_RATIO_TARGET)\n",
    "\n",
    "    # Train\n",
    "    fs_model = LGBReg(**lgb_fs_params).fit(train_fs)\n",
    "    \n",
    "    # Score\n",
    "    val_scored_fs = fs_model.transform(\n",
    "        val_fs.select(\n",
    "            \"features\",\n",
    "            F.col(LOSS_RATIO_TARGET).alias(\"y\"),\n",
    "            F.col(\"model_weight\").alias(\"w\"),\n",
    "        )\n",
    "    ).select(\n",
    "        F.col(\"prediction\").alias(\"yhat\"),\n",
    "        F.col(\"y\"),\n",
    "        F.col(\"w\"),\n",
    "    )\n",
    "    # Metrics vs baseline\n",
    "    val_mets_fs = metrics_sdf(\n",
    "                    sdf = val_scored_fs,\n",
    "                    label = \"y\",\n",
    "                    pred = \"yhat\",\n",
    "                    w=\"w\",\n",
    "                )\n",
    "\n",
    "    mlflow.log_metric(\"fs_fixed_val_mae\",  float(val_mets_fs[\"mae_w\"]))\n",
    "    mlflow.log_metric(\"fs_fixed_val_mse\",  float(val_mets_fs[\"mse_w\"]))\n",
    "    mlflow.log_metric(\"fs_fixed_val_rmse\", float(val_mets_fs.get(\"rmse_w\", 0.0)))\n",
    "\n",
    "    print(\"[FS] Fixed model (all features) – val metrics:\")\n",
    "    print(\"      mae          =\", val_mets_fs[\"mae_w\"])\n",
    "    print(\"      mse          =\", val_mets_fs[\"mse_w\"])\n",
    "    print(\"      rmse          =\", val_mets_fs[\"rmse_w\"])\n",
    "\n",
    "    k_results = []\n",
    "    k_results.append((\"ALL\", val_mets_fs[\"mae_w\"], None))\n",
    "\n",
    "    # Build feature importance table at index + base_feature level\n",
    "    importances = fs_model.getFeatureImportances()\n",
    "\n",
    "    fi_idx = []\n",
    "    for idx, imp in enumerate(importances):\n",
    "        if imp is None:\n",
    "            continue\n",
    "        name = index_to_name.get(idx, f\"idx_{idx}\")\n",
    "        fi_idx.append((idx, name, float(imp)))\n",
    "\n",
    "    fi_idx_df = spark.createDataFrame(fi_idx, [\"idx\", \"feature_name\", \"importance\"])\n",
    "\n",
    "    # Normalized importance over all dims\n",
    "    total_imp = fi_idx_df.agg(F.sum(\"importance\")).first()[0]\n",
    "    fi_idx_df = fi_idx_df.withColumn(\n",
    "        \"importance_norm\",\n",
    "        F.col(\"importance\") / F.lit(total_imp)\n",
    "    )\n",
    "\n",
    "    # Collapse OHE levels to base_feature\n",
    "    fi_idx_df = fi_idx_df.withColumn(\n",
    "        \"base_feature\",\n",
    "        strip_ohe_suffix_udf(F.col(\"feature_name\"))\n",
    "    )\n",
    "\n",
    "    fi_base = (\n",
    "        fi_idx_df\n",
    "        .groupBy(\"base_feature\")\n",
    "        .agg(\n",
    "            F.sum(\"importance_norm\").alias(\"importance_sum\"),\n",
    "            F.avg(\"importance_norm\").alias(\"importance_mean\"),\n",
    "            F.countDistinct(\"idx\").alias(\"n_indices\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"importance_sum\").desc())\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"fs_n_dims\", fi_idx_df.count())\n",
    "    mlflow.log_metric(\"fs_n_base_features\", fi_base.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b4a5c9-141a-45e4-a98f-ab92a4f5d6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7) Select Top K Features for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735b0528-9631-48b4-a461-f60d7a339b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with Top 50 Features\n",
      "K=50: 50 base features → 323 vector indices\n",
      "Training LightGBM model for K=50\n",
      "K=50 → val_mae=0.081251\n",
      "Evaluating with Top 75 Features\n",
      "K=75: 75 base features → 403 vector indices\n",
      "Training LightGBM model for K=75\n",
      "K=75 → val_mae=0.081219\n",
      "Evaluating with Top 150 Features\n",
      "K=150: 150 base features → 631 vector indices\n",
      "Training LightGBM model for K=150\n",
      "K=150 → val_mae=0.081142\n",
      "Evaluating with Top 300 Features\n",
      "K=300: 300 base features → 954 vector indices\n",
      "Training LightGBM model for K=300\n",
      "K=300 → val_mae=0.081207\n",
      "Best K=150 with val_mae=0.081142\n"
     ]
    }
   ],
   "source": [
    "# Evaluate candidate top-K sets\n",
    "candidate_ks = [50, 75, 150, 300]\n",
    "fs_by_k      = {}\n",
    "\n",
    "for k in candidate_ks:\n",
    "    print(f\"Evaluating with Top {k} Features\")\n",
    "\n",
    "    # Top-K base features by importance_sum\n",
    "    top_k_base = (\n",
    "        fi_base.orderBy(F.col(\"importance_sum\").desc())\n",
    "                .limit(k)\n",
    "                .select(\"base_feature\")\n",
    "                .rdd.flatMap(lambda r: r)\n",
    "                .collect()\n",
    "    )\n",
    "\n",
    "    # All indices where base_feature is in that top-K set\n",
    "    indices_for_k = (\n",
    "        fi_idx_df\n",
    "        .where(F.col(\"base_feature\").isin(top_k_base))\n",
    "        .select(\"idx\")\n",
    "        .rdd.flatMap(lambda r: r)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    indices_for_k = sorted(set(indices_for_k))\n",
    "    fs_by_k[k] = indices_for_k\n",
    "\n",
    "    print(f\"K={k}: {len(top_k_base)} base features → {len(indices_for_k)} vector indices\")\n",
    "\n",
    "    mlflow.log_metric(f\"fs_topk_{k}_n_base_features\", float(len(top_k_base)))\n",
    "    mlflow.log_metric(f\"fs_topk_{k}_n_indices\",       float(len(indices_for_k)))\n",
    "\n",
    "    # Slice features for train/val for this K\n",
    "    slicer_k = VectorSlicer(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"features_fs\",\n",
    "        indices=indices_for_k,\n",
    "    )\n",
    "\n",
    "    tr_k = slicer_k.transform(train_fs).drop(\"features\").withColumnRenamed(\"features_fs\", \"features\")\n",
    "    va_k = slicer_k.transform(val_fs).drop(\"features\").withColumnRenamed(\"features_fs\", \"features\")\n",
    "\n",
    "    # Fixed LightGBM model for scoring with K features\n",
    "    print(f\"Training LightGBM model for K={k}\")\n",
    "    lr_k = LGBReg(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=LOSS_RATIO_TARGET,\n",
    "        weightCol=\"model_weight\",\n",
    "        objective=\"regression\",\n",
    "        numIterations=150,\n",
    "        learningRate=0.05,\n",
    "        numLeaves=31,\n",
    "        maxDepth=-1,\n",
    "        baggingFraction=0.8,\n",
    "        featureFraction=0.8,\n",
    "        seed=RANDOM_SEED,\n",
    "    ).fit(tr_k)\n",
    "\n",
    "    va_scored_k = lr_k.transform(\n",
    "        va_k.select(\n",
    "            \"features\",\n",
    "            F.col(LOSS_RATIO_TARGET).alias(\"y\"),\n",
    "            F.col(\"model_weight\").alias(\"w\")\n",
    "            ) \n",
    "        ).select(\n",
    "            F.col(\"prediction\").alias(\"yhat\"),\n",
    "            F.col(\"y\"),\n",
    "            F.col(\"w\"),\n",
    "        )\n",
    "\n",
    "\n",
    "    mets_k = metrics_sdf(\n",
    "        sdf = va_scored_k,\n",
    "        label = \"y\",\n",
    "        pred = \"yhat\",\n",
    "        w = \"w\",\n",
    "    )\n",
    "\n",
    "    print(f\"K={k} → val_mae={mets_k['mae_w']:.6f}\")\n",
    "    mlflow.log_metric(f\"fs_topk_{k}_val_mae\", float(mets_k[\"mae_w\"]))\n",
    "    if \"rmse_w\" in mets_k:\n",
    "        mlflow.log_metric(f\"fs_topk_{k}_val_rmse\", float(mets_k[\"rmse_w\"]))\n",
    "\n",
    "    k_results.append((k, mets_k[\"mae_w\"], indices_for_k))\n",
    "\n",
    "# Choose best K by val_mae_w, slice final features on dfe_tr/va/te\n",
    "k_results_sorted = sorted(k_results, key=lambda t: t[1])\n",
    "best_K, best_mae, best_indices = k_results_sorted[0]\n",
    "\n",
    "print(f\"Best K={best_K} with val_mae={best_mae:.6f}\")\n",
    "mlflow.log_param(\"fs_best_K\", int(best_K))\n",
    "mlflow.log_metric(\"fs_best_K_val_mae\", float(best_mae))\n",
    "mlflow.log_metric(\"fs_best_K_n_indices\", float(len(best_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aec1949-6e0c-4cf2-87cb-a27e444da42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8) CV & Best Regression Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd80210-21d6-4675-b7b8-830115c80e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    }
   ],
   "source": [
    "# CV to tune hyperparams and select best model\n",
    "\n",
    "# Final slicing for full train/val/test\n",
    "final_slicer = VectorSlicer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"features_fs\",\n",
    "    indices=best_indices,\n",
    ")\n",
    "\n",
    "dfe_tr_r = final_slicer.transform(dfe_tr).drop(\"features\").withColumnRenamed(\"features_fs\", \"features\")\n",
    "dfe_va_r = final_slicer.transform(dfe_va).drop(\"features\").withColumnRenamed(\"features_fs\", \"features\")\n",
    "dfe_te_r = final_slicer.transform(dfe_te).drop(\"features\").withColumnRenamed(\"features_fs\", \"features\")\n",
    "\n",
    "tr_sub = dfe_tr_r.where(F.col(LOSS_RATIO_TARGET).isNotNull())\n",
    "va_sub = dfe_va_r.where(F.col(LOSS_RATIO_TARGET).isNotNull())\n",
    "\n",
    "tr = tr_sub.select(\n",
    "    \"features\",\n",
    "    F.col(LOSS_RATIO_TARGET).alias(\"label\"),\n",
    "    F.col(\"model_weight\").alias(\"weight\"),\n",
    ")\n",
    "va = va_sub.select(\n",
    "    \"features\",\n",
    "    F.col(LOSS_RATIO_TARGET).alias(\"label\"),\n",
    "    F.col(\"model_weight\").alias(\"weight\"),\n",
    ")\n",
    "\n",
    "# Fit cv\n",
    "lib_tag, model, score = best_of_trials(\n",
    "    tr,\n",
    "    va,\n",
    "    \"label\",\n",
    "    \"weight\",\n",
    "    run_prefix=f\"lr_cv\",\n",
    "    objective=(\"squared\"),\n",
    ")\n",
    "\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a348f6c9-7fac-4c43-9c3d-14f9253fd605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9) Register Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ec0196-e36b-455f-8199-183ce7ff4f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 17:50:46 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n",
      "2025/11/17 17:51:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2359134503395794/ebe691c423874736b909e7e034a74d5a/artifacts/capstone_model_final/sparkml, flavor: spark), fall back to return ['pyspark==3.4.1']. Set logging level to DEBUG to see the full traceback.\n",
      "Successfully registered model 'capstone_model_final'.\n",
      "2025/11/17 17:51:31 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: capstone_model_final, version 1\n",
      "Created version '1' of model 'capstone_model_final'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"register best model\"):\n",
    "    log_model(\n",
    "        spark_model=best_model,\n",
    "        artifact_path=\"capstone_model_final\",\n",
    "        registered_model_name=\"capstone_model_final\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d0f15e-03fd-47d7-9905-88bf81c6c599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10) Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b686d10-c217-4457-a772-3285067c3113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [w] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts under: /FileStore/TB/Capstone_Project/img\n",
      "\n",
      "[LR] Test regression metrics: {'mae_w': 0.08393868865272433, 'mse_w': 0.080602327222287, 'rmse_w': 0.2839054899474242}\n"
     ]
    }
   ],
   "source": [
    "reports = {}\n",
    "\n",
    "reports[\"lr\"] = eval_and_save(\"lr\", LOSS_RATIO_TARGET, \"lr_pred\")\n",
    "\n",
    "print(\"Artifacts under:\", DBFS_FILES_BASE)\n",
    "print(\"\\n[LR] Test regression metrics:\", reports[\"lr\"][\"mets\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5229529044839431,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Audience_Intelligence_Model_Final_Masked",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
