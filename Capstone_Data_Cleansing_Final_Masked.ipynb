{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f66cbd2-b2c8-4efc-9bef-31a208dd60b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc965609-3083-4b0c-a2fe-56c1467a166c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "290cce23-209d-4bd9-b23e-566ca3a8df02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0ae698-4364-40a3-b483-415b97724bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.table(\"XXX\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf84181-4a71-4c6a-809f-14e2685e2607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2) Recoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab08dd77-9e76-4cb1-81dd-9c99815eb4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Derive date from datepart columns and convert to date type\n",
    "def first_day_of_period_expr(colname: str):\n",
    "    v = F.regexp_replace(F.col(colname), r\"\\D\", \"\")\n",
    "    L = F.length(v)\n",
    "\n",
    "    year = v.substr(1, 4)\n",
    "    mm   = v.substr(5, 2)\n",
    "    dd   = v.substr(7, 2)\n",
    "\n",
    "    # quarter â†’ first month\n",
    "    q     = v.substr(5, 1).cast(\"int\")\n",
    "    q_mm  = F.lpad(((q - 1) * 3 + 1).cast(\"string\"), 2, \"0\")\n",
    "\n",
    "    date_str = (\n",
    "        F.when(L == 8, F.when(dd == \"00\", F.concat(year, mm, F.lit(\"01\"))).otherwise(v)) # YYYYMMDD or YYYYMM00\n",
    "         .when(L == 6, F.concat(year, mm, F.lit(\"01\")))     # YYYYMM\n",
    "         .when(L == 5, F.concat(year, q_mm, F.lit(\"01\")))   # YYYYQ\n",
    "         .when(L == 4, F.concat(year, F.lit(\"0101\")))         # YYYY\n",
    "         .otherwise(F.lit(None))\n",
    "    )\n",
    "    return F.to_date(date_str, \"yyyyMMdd\")\n",
    "\n",
    "for c in datepart_cols:\n",
    "     df = df.withColumn(c, first_day_of_period_expr(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b8f5a1-16ff-4cf4-b754-7f4d96624c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Decode column names\n",
    "df_col_lookup = spark.sql(\"SELECT * FROM XXX\")\n",
    "\n",
    "col_map = {row['Header Name']: row['Name'] for row in df_col_lookup.collect() if 'Header Name' in row.asDict() and 'Name' in row.asDict()}\n",
    "for old, new in col_map.items():\n",
    "    if old in df.columns:\n",
    "        df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44591f56-d29c-47a0-8cf2-c1d1fa5e6beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3) Header Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e86889-6718-481e-bcf8-a04c4ffc8b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.toDF(*[c.replace(\".\", \"_\")\\\n",
    "                .replace(\" \", \"_\")\\\n",
    "                .replace(\"/\", \"\")\\\n",
    "                .replace(\"%\", \"pct\")\\\n",
    "                .replace(\"(\", \"\")\\\n",
    "                .replace(\")\", \"\")\\\n",
    "                .replace(\"[\", \"\")\\\n",
    "                .replace(\"[\", \"\")\\\n",
    "                .replace(\"-\", \"\")\\\n",
    "                .replace(\",\", \"\")\\\n",
    "                .replace(\";\", \"\")\\\n",
    "                .replace(\":\", \"\")\\\n",
    "                .replace(\"{\", \"\")\\\n",
    "                .replace(\"}\", \"\")\\\n",
    "                .replace(\"=\", \"\")\\\n",
    "                .replace(\"\\xa0\", \"_\")\\\n",
    "                                            \n",
    "                for c in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3521fbbb-77cd-4964-af2d-9c030521f6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4) Missingness Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6db182-ac89-4664-ba3d-f38ff8413a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Null profile\n",
    "n_rows = df.count()\n",
    "\n",
    "null_exprs = [\n",
    "    (F.count(F.when(F.col(c).isNull(), c))/n_rows).alias(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "\n",
    "null_df = df.agg(*null_exprs)\n",
    "\n",
    "null_dist = (null_df.select(F.explode(F.map_from_arrays(F.array([F.lit(c) for c in null_df.columns]), F.array(*[F.col(c) for c in null_df.columns]))).alias(\"col\", \"pct_null\")))\n",
    "\n",
    "result = null_dist.orderBy(F.desc(\"pct_null\"))\n",
    "\n",
    "cols = [row[\"col\"] for row in null_dist.filter(F.col(\"pct_null\")>0.5).collect()]\n",
    "\n",
    "# Drop sparse columns\n",
    "df = df.drop(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265efd2e-457c-4281-925c-708c38ed889f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5) Constant Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2dd51b0-f78a-446c-bab6-1e52161b8876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Constant columns\n",
    "\n",
    "# Keep only orderable cols\n",
    "orderable = {T.StringType, T.IntegerType, \n",
    "             T.DateType, T.DoubleType, T.LongType, T.FloatType} \n",
    "             \n",
    "cols = [f.name for f in df.schema.fields if type(f.dataType) in orderable]\n",
    "\n",
    "aggs = []\n",
    "for c in cols:\n",
    "    aggs += [\n",
    "        F.min(F.col(c)).alias(f\"{c}__min\"),\n",
    "        F.max(F.col(c)).alias(f\"{c}__max\"),\n",
    "    ]\n",
    "\n",
    "stats = df.agg(*aggs).collect()[0].asDict()\n",
    "\n",
    "# Drop col where min = max\n",
    "const_cols = [c for c in cols\n",
    "              if stats[f\"{c}__min\"] == stats[f\"{c}__max\"]]\n",
    "\n",
    "df = df.drop(*const_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444b1a34-7165-43e9-8cd2-583345cbcd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6) Impute Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244f0320-f833-428d-afd2-47c13f05e9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [f.name for f in df.schema.field if isinstance(f.dataType, T.StringType)]\n",
    "\n",
    "cat_imputer = df.fillna({c: \"__MISSING__\" for c in cat_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3955885f-ae58-48e5-85d7-31d6aa743e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7) Colinearity Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30447c21-c2c6-4218-97df-4ef6e93cc441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [f.name for f in cat_imputer.schema.fields if isinstance(f.dataType, T.NumericType)]\n",
    "\n",
    "df_num = df.select(*[F.col(c).cast(\"double\").alias(c) for c in numeric_cols])\n",
    "\n",
    "meds = (df_num.select([\n",
    "            F.expr(f'percentile_approx(`{c}`, 0.5, {int(1/0.01)})')\n",
    "            .alias(c) for c in df_num.columns])\n",
    "            .first().asDict())\n",
    "\n",
    "df_work = df_num.fillna(meds)\n",
    "\n",
    "df_work = df_work.cache(); _ = df_work.count()\n",
    "\n",
    "nn_row = (df_num.agg(*[\n",
    "            F.sum(F.when(F.col(c).isNotNull(), 1)\n",
    "            .otherwise(0)).alias(c) for c in df_num.columns])\n",
    "            .collect()[0].asDict())\n",
    "        \n",
    "n_total = df_num.count()\n",
    "missing_frac = {c: 1.0 - (nn_row.get(c, 0) / max(1, n_total)) for c in df_num.columns}\n",
    "\n",
    "va = VectorAssembler(inputCols=df_num.columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "vecdf = va.transform(df_work).select(\"features\").cache(); _ = vecdf.count()\n",
    "\n",
    "corr_mat = Correlation.corr(vecdf, \"features\", method=\"pearson\").head()[0].toArray()\n",
    "abs_corr = np.abs(corr_mat)\n",
    "\n",
    "to_drop = set() \n",
    "cols = df_num.columns \n",
    "upper = np.triu(abs_corr, k=1) \n",
    "pairs = np.argwhere(upper >= 0.90) \n",
    "pairs = sorted(pairs, key=lambda ij: upper[ij[0], ij[1]], reverse=True)\n",
    "\n",
    "for i, j in pairs:\n",
    "    c1, c2 = cols[i], cols[j]\n",
    "    if c1 in to_drop or c2 in to_drop:\n",
    "        continue\n",
    "    m1, m2 = missing_frac.get(c1, 0.0), missing_frac.get(c2, 0.0)\n",
    "    drop = c1 if (m1 > m2 or (m1 == m2 and len(c1) >= len(c2))) else c2\n",
    "    to_drop.add(drop)\n",
    "\n",
    "df = df.drop(*to_drop).cache()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5103862795752222,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_Data_Cleansing_Final_Masked",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
